{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Example of a bayesian classifier\n",
    " \n",
    " \n",
    " First is just reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "from numpy import array, concatenate, zeros, ones\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with open(\"wineX.txt\") as data_file:\n",
    "    for line in data_file.read().split(\"\\n\"):\n",
    "        l = []\n",
    "        for item in line.split(\",\"):\n",
    "            if item != \"\":\n",
    "                l.append(float(item))\n",
    "            \n",
    "        if len(l) > 5:\n",
    "            data.append(l)\n",
    "        \n",
    "\n",
    "with open(\"wineY.txt\") as label_file:\n",
    "    tmp = label_file.readlines()\n",
    "    \n",
    "    for label in tmp:\n",
    "        labels.append(int(label[0]))\n",
    "    \n",
    "    \n",
    "traind, testd, trainl, testl = train_test_split(data, labels, test_size=0.60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 0.0008151531219482422\n",
      "Correct rate: 0.9626168224299065\n",
      "Avarage predict rate: 9.776498669775847e-05\n"
     ]
    }
   ],
   "source": [
    "from naive_bayes import classifier\n",
    "from time import time\n",
    "from numpy import arange, array\n",
    "\n",
    "traind = array(traind)\n",
    "trainl = array(trainl)\n",
    "\n",
    "testd = array(testd)\n",
    "testl = array(testl)\n",
    "\n",
    "c = classifier()\n",
    "\n",
    "timestamp = time()\n",
    "c.train(traind, trainl)\n",
    "print(\"Time to train: {}\".format(time() - timestamp))\n",
    "\n",
    "timestamp = time()\n",
    "correct = 0\n",
    "for index, t in enumerate(testd):\n",
    "    correct += (c.predict(t) == testl[index])\n",
    "\n",
    "print(\"Correct rate: {}\".format(correct / len(testd)))\n",
    "print(\"Avarage predict rate: {}\".format((time() - timestamp) / len(testd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Adaboosting on Bayesian Classifier\n",
    "\n",
    "Ada boost is a ensemble learning method.\n",
    "Given X weak classifiers we can combine them into a strong classifier\n",
    "by weigthing them to predict on features they're good at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 0.07660698890686035\n"
     ]
    }
   ],
   "source": [
    "from numpy import array, ones, log\n",
    "from time import time\n",
    "from naive_bayes import classifier\n",
    "from math import pow\n",
    "\n",
    "traind = array(traind)\n",
    "trainl = array(trainl)\n",
    "\n",
    "testd = array(testd)\n",
    "testl = array(testl)\n",
    "\n",
    "# Uniformly distrubte weights on all test data to start with\n",
    "weigths = ones(trainl.shape) / len(trainl)\n",
    "\n",
    "# List of weights describing voting strenght for each classifier\n",
    "alphas = []\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = []\n",
    "\n",
    "# Number of classifiers we want\n",
    "X = 10\n",
    "\n",
    "timestamp = time()\n",
    "for ci in range(X):\n",
    "    \n",
    "    # Train a Classifier\n",
    "    c = classifier().train(traind, trainl, weigths)\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the error of the classifier on the training data.\n",
    "    \n",
    "    This can be done in multiple ways, one example is the\n",
    "    sum of all the weights of the incorrectly classified samples.\n",
    "    \"\"\" \n",
    "    error = 0\n",
    "    \n",
    "    # Remember what it classified correctly.\n",
    "    classifications = []\n",
    "    for i, t in enumerate(traind):\n",
    "        prediction = c.predict(t) != trainl[i]\n",
    "        error += weigths[i] * prediction\n",
    "        classifications.append(prediction)\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply some non-linearity to it to give it a low value\n",
    "    if the error is low.\n",
    "    \"\"\" \n",
    "    if error == 0:\n",
    "        NLE = 1e-10\n",
    "    else:\n",
    "        NLE = error / (1 - error)\n",
    "    \n",
    "    \"\"\"\n",
    "    Update weights in such a way that the \n",
    "    classifier is encouraged to learn the features\n",
    "    this classifier failed at. \n",
    "    \n",
    "    Either increase the weights of the wrongly classified\n",
    "    or reduce the weigths of the correctly classified.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    uw = zeros(weigths.shape)\n",
    "    # This reduces weigths of correctly classified\n",
    "    for i, w in enumerate(weigths):\n",
    "        uw[i] = w * pow(NLE, 1 - classifications[i])\n",
    "    \n",
    "    weigths = uw\n",
    "    \"\"\"\n",
    "    The alpha should be inversly proportionate to the error.\n",
    "    \n",
    "    High if there is few errors, low otherwise.\n",
    "    \"\"\" \n",
    "    # Log to normalize it\n",
    "    alphas.append(log(1 / NLE))\n",
    "\n",
    "    # Normalize weigths \n",
    "    weigths = weigths / sum(weigths)\n",
    "    \n",
    "    classifiers.append(c)\n",
    "\n",
    "print(\"Time to train: {}\".format(time() - timestamp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using ada Boost\n",
    "\n",
    "    Classification is done by voting and can be implemented in\n",
    "    a multitude of ways. The principle is to let each classifier\n",
    "    cast its vote and choose whichever class gets the most votes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate: 0.9626168224299065\n",
      "Avarage predict rate: 0.0010540864177953418\n"
     ]
    }
   ],
   "source": [
    "timestamp = time()\n",
    "correct = 0\n",
    "for index, t in enumerate(testd):\n",
    "    votes = {}\n",
    "    for idx, classifier in enumerate(classifiers):\n",
    "        vote = classifier.predict(t)\n",
    "        \n",
    "        if vote in votes:\n",
    "            votes[vote] += alphas[idx]\n",
    "        else:\n",
    "            votes[vote] = 0\n",
    "    \n",
    "    mxv = 0\n",
    "    key = None\n",
    "    for k, v in votes.items():\n",
    "        if v > mxv:\n",
    "            mxv = v\n",
    "            key = k\n",
    "      \n",
    "    correct += (key == testl[index])\n",
    "    \n",
    "print(\"Correct rate: {}\".format(correct / len(testd)))\n",
    "print(\"Avarage predict rate: {}\".format((time() - timestamp) / len(testd)))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
